% This file was created with JabRef 2.7.2.
% Encoding: UTF-8

% 1. 文科类各教学单位对设计（论文）的引、注、参考文献的著录要求，由各教学单位根据学科特点自行制订统一、规范的具体要求，并报教务处备案。
% 2. 理工农医各专业毕业实习、毕业设计（论文）参考文献著录要求：引用资料、文献，均应说明来源。著录引文的参考文献采用顺序编码制。
% 3. 顺序编码制：按文章正文部分（包括图、表及其说明）引用文献的先后顺序连续编码。
%    编码置于方括号中，用上标的形式（置于右上角），直接放在引文之后（如［1］；［15,18］；［25－26］）。

% 主要责任者者.书名.其他责任者.版本.出版地：出版者,出版年：页次

@article{baldominos_survey_2019,
  title = {A {{Survey}} of {{Handwritten Character Recognition}} with {{MNIST}} and {{EMNIST}}},
  shorttitle = {{{MNIST和EMNIST手写字符识别综述}}},
  author = {Baldominos, Alejandro and Saez, Yago and Isasi, Pedro},
  year = {2019},
  month = aug,
  journal = {Applied Sciences},
  volume = {9},
  number = {15},
  pages = {3169},
  issn = {2076-3417},
  doi = {10.3390/app9153169},
  urldate = {2024-04-29},
  abstract = {This paper summarizes the top state-of-the-art contributions reported on the MNIST dataset for handwritten digit recognition. This dataset has been extensively used to validate novel techniques in computer vision, and in recent years, many authors have explored the performance of convolutional neural networks (CNNs) and other deep learning techniques over this dataset. To the best of our knowledge, this paper is the first exhaustive and updated review of this dataset; there are some online rankings, but they are outdated, and most published papers survey only closely related works, omitting most of the literature. This paper makes a distinction between those works using some kind of data augmentation and works using the original dataset out-of-the-box. Also, works using CNNs are reported separately; as they are becoming the state-of-the-art approach for solving this problem. Nowadays, a significant amount of works have attained a test error rate smaller than 1\% on this dataset; which is becoming non-challenging. By mid-2017, a new dataset was introduced: EMNIST, which involves both digits and letters, with a larger amount of data acquired from a database different than MNIST's. In this paper, EMNIST is explained and some results are surveyed. 【摘要翻译】本文总结了MNIST数据集对手写数字识别的最先进贡献。该数据集已被广泛用于验证计算机视觉中的新技术，近年来，许多作者探索了卷积神经网络（CNN）和其他深度学习技术在该数据集上的性能。据我们所知，本文是对该数据集的第一次详尽和更新的回顾；有一些在线排名，但它们已经过时，大多数已发表的论文只调查密切相关的作品，省略了大部分文献。本文区分了那些使用某种数据增强的作品和使用开箱即用原始数据集的作品。此外，使用CNN的作品被单独报告；因为它们正在成为解决这个问题的最先进方法。如今，在这个数据集上，大量的工作已经达到了小于1\%的测试错误率；这变得没有挑战性。到2017年年中，引入了一个新的数据集：EMNIST，它涉及数字和字母，从不同于MNIST的数据库中获取了大量数据。在本文中，对EMNIST进行了解释，并对一些结果进行了调查。},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  lccn = {4},
  keywords = {/unread},
  annotation = {🏷️ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/9BAD4LZJ/Baldominos et al. - 2019 - A Survey of Handwritten Character Recognition with MNIST and EMNIST.pdf}
}

@misc{google_tensorflow_2023,
  title = {Tensorflow {{C API}}},
  author = {Google},
  year = {2023},
  month = nov,
  keywords = {/unread},
  annotation = {🏷️ /unread}
}

@article{ho_denoising_nodate,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  shorttitle = {去噪扩散概率模型},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal = {arXiv},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion. 【摘要翻译】我们使用扩散概率模型呈现高质量的图像合成结果，这是一类受非平衡热力学考虑启发的潜在变量模型。我们的最佳结果是通过在加权变分界上训练获得的，该变分界是根据扩散概率模型和与朗之万动力学的去噪分数匹配之间的新颖联系而设计的，我们的模型自然承认一种渐进的有损解压缩方案，可以解释为自回归解码的泛化。在无条件的CIFAR10数据集上，我们获得了9.46的初始分数和3.17的最先进的FID分数。在256x256 LSUN上，我们获得了类似于ProgressiveGAN的样本质量。我们的实现可在https://github.com/hojonathanho/diffusion获得。},
  langid = {english},
  keywords = {/unread,diffusion model},
  annotation = {🏷️ /unread、diffusion model},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/9PHVM6HW/Ho et al. - Denoising Diffusion Probabilistic Models.pdf}
}

@misc{li_studying_2022,
  title = {Studying {{Popular Open Source Machine Learning Libraries}} and {{Their Cross-Ecosystem Bindings}}},
  shorttitle = {研究流行的开源机器学习库及其跨生态系统绑定},
  author = {Li, Hao and Bezemer, Cor-Paul},
  year = {2022},
  month = jan,
  number = {arXiv:2201.07201},
  eprint = {2201.07201},
  publisher = {arXiv},
  urldate = {2024-01-07},
  abstract = {Open source machine learning (ML) libraries allow developers to integrate advanced ML functionality into their own applications. However, popular ML libraries, such as TensorFlow, are not available natively in all programming languages and software package ecosystems. Hence, developers who wish to use an ML library which is not available in their programming language or ecosystem of choice, may need to resort to using a so-called binding library. Binding libraries provide support across programming languages and package ecosystems for a source library. For example, the Keras .NET binding provides support for the Keras library in the NuGet (.NET) ecosystem even though the Keras library was written in Python. In this paper, we conduct an in-depth study of 155 cross-ecosystem bindings and their development for 36 popular open source ML libraries. Our study shows that for most popular ML libraries, only one package ecosystem is officially supported (usually PyPI). Cross-ecosystem support, which is available for 25\% of the studied ML libraries, is usually provided through community-maintained bindings, e.g., 73\% of the bindings in the npm ecosystem are community-maintained. Our study shows that the vast majority of the studied bindings cover only a small portion of the source library releases, and the delay for receiving support for a source library release is large. 【摘要翻译】开源机器学习（ML）库允许开发人员将高级ML功能集成到自己的应用程序中。但是，流行的ML库（例如TensorFlow）在所有编程语言和软件包生态系统中都不可用。因此，希望使用ML库的开发人员可能需要诉诸于使用所谓的绑定库。绑定库为源库提供了跨编程语言和包装生态系统的支持。例如，即使KERAS库是用Python编写的，Keras .NET绑定也为Nuget（.NET）生态系统中的Keras库提供了支持。在本文中，我们对36个流行的开源ML库进行了155个跨生态系统绑定及其开发的深入研究。我们的研究表明，对于最受欢迎的ML库，只有一个包装生态系统受到了支持（通常是PYPI）。通常通过社区维护的绑定提供的跨生态系统支持，可用于研究的ML库中25％，例如，NPM生态系统中73％的绑定是社区维护的。我们的研究表明，绝大多数研究的绑定仅涵盖源库的一小部分，并且接收对源库释放的支持的延迟很大。},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {/unread,Computer Science - Machine Learning,Computer Science - Software Engineering},
  annotation = {🏷️ /unread、Computer Science - Machine Learning、Computer Science - Software Engineering},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/VA8S4TAW/Li and Bezemer - 2022 - Studying Popular Open Source Machine Learning Libraries and Their Cross-Ecosystem Bindings.pdf}
}

@inproceedings{macukow_neural_2016,
  title = {Neural Networks -- State of Art, Brief History, Basic Models and Architecture},
  shorttitle = {神经网络-最先进的技术、简史、基本模型和架构},
  booktitle = {Computer Information Systems and Industrial Management},
  author = {Macukow, Bohdan},
  editor = {Saeed, Khalid and Homenda, W{\l}adys{\l}aw},
  year = {2016},
  pages = {3--14},
  publisher = {Springer International Publishing},
  address = {Cham},
  abstract = {The history of neural networks can be traced back to the work of trying to model the neuron. Today, neural networks discussions are occurring everywhere. Neural networks, with their remarkable ability to derive meaning from complicated or imprecise data, can be used to extract patterns and detect trends that are too complex to be noticed by either humans or other computer techniques. A brief history of the neural networks research is presented and some more popular models are briefly discussed. The major attention is on the feed-forward networks and specially to the topology of such the network and method of building the multi-layer perceptrons. 【摘要翻译】神经网络的历史可以追溯到试图模拟神经元的工作。今天，神经网络的讨论无处不在。神经网络具有从复杂或不精确的数据中获得意义的非凡能力，可用于提取模式和检测过于复杂而无法被人类或其他计算机技术注意到的趋势。介绍了神经网络研究的简史，并简要讨论了一些更流行的模型。主要关注的是前馈网络，特别是这种网络的拓扑和构建多层感知器的方法。},
  isbn = {978-3-319-45378-1},
  langid = {english},
  keywords = {/unread},
  annotation = {🏷️ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/E53X8XL2/978-3-319-45378-1_1.pdf}
}

@misc{pytorch_pytorch_2022,
  title = {{{PYTORCH C}}++ {{API}}},
  author = {PyTorch, Contributors},
  year = {2022},
  publisher = {Pytorch},
  copyright = {{\copyright} Copyright 2022, PyTorch Contributors},
  langid = {english},
  keywords = {/unread},
  annotation = {🏷️ /unread}
}

@article{saha_machine_2022,
  title = {Machine {{Learning}} for {{Microcontroller-Class Hardware}}: {{A Review}}},
  shorttitle = {微控制器级硬件的机器学习：评论},
  author = {Saha, Swapnil Sayan and Sandha, Sandeep Singh and Srivastava, Mani},
  year = {2022},
  month = nov,
  journal = {IEEE Sensors Journal},
  volume = {22},
  number = {22},
  pages = {21362--21390},
  issn = {1530-437X},
  doi = {10.1109/JSEN.2022.3210773},
  urldate = {2024-01-07},
  abstract = {The advancements in machine learning (ML) opened a new opportunity to bring intelligence to the low-end Internet-of-Things (IoT) nodes, such as microcontrollers. Conventional ML deployment has high memory and computes footprint hindering their direct deployment on ultraresource-constrained microcontrollers. This article highlights the unique requirements of enabling onboard ML for microcontroller-class devices. Researchers use a specialized model development workflow for resource-limited applications to ensure that the compute and latency budget is within the device limits while still maintaining the desired performance. We characterize a closed-loop widely applicable workflow of ML model development for microcontroller-class devices and show that several classes of applications adopt a specific instance of it. We present both qualitative and numerical insights into different stages of model development by showcasing several use cases. Finally, we identify the open research challenges and unsolved questions demanding careful considerations moving forward. 【摘要翻译】机器学习的进步（ML）为低端互联网（IoT）节点（例如微控制器）带来了新的机会，将情报带入了智能。传统的ML部署具有很高的记忆力，并计算占地面积，阻碍了其在超大限制的微控制器上的直接部署。本文重点介绍了为微控制器级设备启用板载ML的独特要求。研究人员使用专门的模型开发工作流进行资源有限的应用程序，以确保计算和延迟预算在设备限制之内，同时仍保持所需的性能。我们表征了微控制器级设备的ML模型开发的广泛适用的闭环工作流，并表明几类应用程序采用了它的特定实例。我们通过展示多种用例，向模型开发的不同阶段提供定性和数值见解。最后，我们确定了开放的研究挑战和未解决的问题，要求仔细考虑前进。},
  langid = {english},
  lccn = {2},
  keywords = {/unread},
  annotation = {🏷️ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/ESFJ9MBU/Saha et al. - 2022 - Machine Learning for Microcontroller-Class Hardware A Review.pdf}
}

@article{sakr_machine_2020,
  title = {Machine {{Learning}} on {{Mainstream Microcontrollers}}},
  shorttitle = {主流微控制器上的机器学习},
  author = {Sakr, Fouad and Bellotti, Francesco and Berta, Riccardo and De Gloria, Alessandro},
  year = {2020},
  month = may,
  journal = {Sensors},
  volume = {20},
  number = {9},
  pages = {2638},
  issn = {1424-8220},
  doi = {10.3390/s20092638},
  urldate = {2024-01-07},
  abstract = {This paper presents the Edge Learning Machine (ELM), a machine learning framework for edge devices, which manages the training phase on a desktop computer and performs inferences on microcontrollers. The framework implements, in a platform-independent C language, three supervised machine learning algorithms (Support Vector Machine (SVM) with a linear kernel, k-Nearest Neighbors (K-NN), and Decision Tree (DT)), and exploits STM X-Cube-AI to implement Artificial Neural Networks (ANNs) on STM32 Nucleo boards. We investigated the performance of these algorithms on six embedded boards and six datasets (four classifications and two regression). Our analysis---which aims to plug a gap in the literature---shows that the target platforms allow us to achieve the same performance score as a desktop machine, with a similar time latency. ANN performs better than the other algorithms in most cases, with no difference among the target devices. We observed that increasing the depth of an NN improves performance, up to a saturation level. k-NN performs similarly to ANN and, in one case, even better, but requires all the training sets to be kept in the inference phase, posing a significant memory demand, which can be afforded only by high-end edge devices. DT performance has a larger variance across datasets. In general, several factors impact performance in different ways across datasets. This highlights the importance of a framework like ELM, which is able to train and compare different algorithms. To support the developer community, ELM is released on an open-source basis. 【摘要翻译】本文介绍了Edge Learning Machine（ELM），这是一个用于Edge设备的机器学习框架，该框架在台式计算机上管理训练阶段，并在微控制器上执行推断。该框架以独立于平台的C语言实现了三种有监督的机器学习算法（支持向量机（SVM），带有线性内核，K-Nearest邻居（K-NN）和决策树（DT）），并利用STM X-Cube-AI在STM32核板上实施人工神经网络（ANN）。我们研究了这些算法在六个嵌入式板和六个数据集上的性能（四个分类和两个回归）。我们的分析（旨在缩小文献中的差距）展示了目标平台使我们能够获得与台式机相同的性能得分，并且具有相似的时间延迟。在大多数情况下，ANN的性能要比其他算法更好，而目标设备之间没有区别。我们观察到，增加NN的深度可提高性能，最高饱和水平。 K-NN的性能与ANN相似，在一种情况下甚至更好，但要求将所有训练集保存在推理阶段，并提出明显的内存需求，只能由高端边缘设备限制。 DT性能在数据集之间具有较大的差异。通常，几个因素会影响整个数据集不同方式的性能。这突出了像ELM这样的框架的重要性，该框架能够训练和比较不同的算法。为了支持开发人员社区，ELM是开源的。},
  langid = {english},
  lccn = {3},
  keywords = {/unread},
  annotation = {🏷️ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/7EQ2EU4D/Sakr et al. - 2020 - Machine Learning on Mainstream Microcontrollers.pdf}
}

@article{sullivan_structure_nodate,
  author = {Sullivan, Kevin J. and Griswold, William G. and Cai, Yuanfang and Hallen, Ben},
  title = {The structure and value of modularity in software design},
  year = {2001},
  issue_date = {Sept. 2001},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {26},
  number = {5},
  issn = {0163-5948},
  url = {https://doi.org/10.1145/503271.503224},
  doi = {10.1145/503271.503224},
  abstract = {The concept of information hiding modularity is a cornerstone of modern software design thought, but its formulation remains casual and its emphasis on changeability is imperfectly related to the goal of creating added value in a given context. We need better explanatory and prescriptive models of the nature and value of information hiding. We evaluate the potential of a new theory---developed to account for the influence of modularity on the evolution of the computer industry---to inform software design. The theory uses design structure matrices to model designs and real options techniques to value them. To test the potential utility of the theory for software we apply it to Parnas's KWIC designs. We contribute an extension to design structure matrices, and we show that the options results are consistent with Parnas's conclusions. Our results suggest that such a theory does have potential to help inform software design.},
  journal = {SIGSOFT Softw. Eng. Notes},
  month = {sep},
  pages = {99–108},
  numpages = {10},
  keywords = {software, real options, modularity, design structure matrix}
}

@inproceedings{tempero_experiment_2023,
  author = {Tempero, Ewan and Blincoe, Kelly and Lottridge, Danielle},
  title = {An Experiment on the Effects of Modularity on Code Modification and Understanding},
  year = {2023},
  isbn = {9781450399418},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3576123.3576138},
  doi = {10.1145/3576123.3576138},
  abstract = {Good modularity is seen as an important goal in software design. Achieving this goal is claimed to improve, among other things, the understandability and modifiability of a design. Yet, when teaching software design, we see that students limit the amount of modularity that they introduce into their code and cannot see the benefit of further modularity. This could be because they do not understand the benefits, but it could also be that these benefits are limited for inexperienced developers. In order to teach the benefits of modularity we need to understand what, if any, benefits exist for students. We conducted a controlled experiment where 40 students performed a modification task on two different designs, one with higher modularity than the other. Students were better able to successfully complete the task with the design with higher modularity. However, we found a trend where understanding was lower for the high modularity design. These results suggest modularity is beneficial to students, and that understanding of modularity needs to be better supported when teaching software design.},
  booktitle = {Proceedings of the 25th Australasian Computing Education Conference},
  pages = {105–112},
  numpages = {8},
  keywords = {code understandability, empirical software engineering, modifiability, modularity},
  location = {<conf-loc>, <city>Melbourne</city>, <state>VIC</state>, <country>Australia</country>, </conf-loc>},
  series = {ACE '23}
}

@article{arkhangelskaya_deep_2023,
  title = {Deep {{Learning}} for {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {自然语言处理的深度学习},
  author = {Arkhangelskaya, E. O. and Nikolenko, S. I.},
  year = {2023},
  month = jul,
  journal = {Journal of Mathematical Sciences},
  volume = {273},
  number = {4},
  pages = {533--582},
  issn = {1072-3374},
  doi = {10.1007/s10958-023-06519-6},
  urldate = {2024-04-29},
  langid = {english},
  keywords = {/unread},
  annotation = {🏷️ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/2968AV86/Arkhangelskaya and Nikolenko - 2023 - Deep Learning for Natural Language Processing A Survey.pdf}
}

@article{akopov_modeling_2023,
  title = {Modeling and Optimization of Strategies for Making Individual Decisions in Multi-Agent Socio-Economic Systems with the Use of Machine Learning},
  shorttitle = {使用机器学习对多智能体社会经济系统中的个人决策策略进行建模和优化},
  author = {Akopov, Andranik},
  year = {2023},
  month = jun,
  journal = {Business Informatics},
  volume = {17},
  number = {2},
  pages = {7--19},
  issn = {2587-814X},
  doi = {10.17323/2587-814X.2023.2.7.19},
  urldate = {2024-04-29},
  abstract = {This article presents a new approach to modeling and optimizing individual decision-making strategies in multi-agent socio-economic systems (MSES). This approach is based on the synthesis of agent-based modeling methods, machine learning and genetic optimization algorithms. A procedure for the synthesis and training of artificial neural networks (ANNs) that simulate the functionality of MSES and provide an approximation of the values of its objective characteristics has been developed. The feature of the two-step procedure is the combined use of particle swarm optimization methods (to determine the optimal values of hyperparameters) and the Adam machine learning algorithm (to compute weight coefficients of the ANN). The use of such ANN-based surrogate models in parallel multi-agent real-coded genetic algorithms (MA-RCGA) makes it possible to raise substantially the time-efficiency of the evolutionary search for optimal solutions. We have conducted numerical experiments that confirm a significant improvement in the performance of MA-RCGA, which periodically uses the ANN-based surrogate-model to approximate the values of the objective and fitness functions. A software framework has been designed that consists of the original (reference) agent-based model of trade interactions, the ANN-based surrogate model and the MA-RCGA genetic algorithm. At the same time, the software libraries FLAME GPU, OpenNN (Open Neural Networks Library), etc., agent-based modeling and machine learning methods are used. The system we developed can be used by responsible managers. 【摘要翻译】本文提出了一种在多智能体社会经济系统（MSES）中建模和优化个人决策策略的新方法。这种方法基于基于智能体的建模方法、机器学习和遗传优化算法的综合。已经开发了一个模拟MSES功能并提供其客观特征值近似值的人工神经网络（ANN）的综合和训练程序。两步程序的特点是结合使用粒子群优化方法（确定超参数的最优值）和亚当机器学习算法（计算ANN的权重系数）。在并行多智能体实数编码遗传算法（MA-RCGA）中使用这种基于ANN的代理模型使得有可能大大提高进化搜索最优解的时间效率。我们进行了数值实验，证实了MA-RCGA性能的显著提高，该算法周期性地使用基于ANN的代理模型来近似目标和适应度函数的值。设计了一个软件框架，由原始的（参考）基于代理的贸易交互模型、基于ANN的代理模型和MA-RCGA遗传算法组成。同时，使用了软件库FLAME GPU、OpenNN（开放神经网络库）等，基于代理的建模和机器学习方法。我们开发的系统可以供负责的管理者使用。},
  langid = {english},
  keywords = {/unread},
  annotation = {🏷️ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/YGV6AJNA/Akopov - 2023 - Modeling and optimization of strategies for making individual decisions in multi-agent socio-economi.pdf}
}

@incollection{pancioni_dll_2018,
  title = {{{DLL}}: {{A Fast Deep Neural Network Library}}},
  shorttitle = {{{DLL}}：一个快速的深度神经网络库},
  booktitle = {Artificial {{Neural Networks}} in {{Pattern Recognition}}},
  author = {Wicht, Baptiste and Fischer, Andreas and Hennebert, Jean},
  editor = {Pancioni, Luca and Schwenker, Friedhelm and Trentin, Edmondo},
  year = {2018},
  volume = {11081},
  pages = {54--65},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2024-04-29},
  abstract = {Deep Learning Library (DLL) is a library for machine learning with deep neural networks that focuses on speed. It supports feedforward neural networks such as fully-connected Artificial Neural Networks (ANNs) and Convolutional Neural Networks (CNNs). Our main motivation for this work was to propose and evaluate novel software engineering strategies with potential to accelerate runtime for training and inference`. Such strategies are mostly independent of the underlying deep learning algorithms. On three different datasets and for four different neural network models, we compared DLL to five popular deep learning libraries. Experimentally, it is shown that the proposed library is systematically and significantly faster on CPU and GPU. In terms of classification performance, similar accuracies as the other libraries are reported. 【摘要翻译】深度学习库（DLL）是一个专注于速度的深度神经网络机器学习库。它支持前馈神经网络，如全连接的人工神经网络（ANN）和卷积神经网络（CNN）。我们这项工作的主要动机是提出和评估新的软件工程策略，这些策略有可能加速训练和推理的运行时间。这些策略大多独立于底层深度学习算法。在三个不同的数据集和四个不同的神经网络模型上，我们将DLL与五个流行的深度学习库进行了比较。实验表明，所提出的库在CPU和GPU上系统且显着地更快。在分类性能方面，报告了与其他库相似的准确性。},
  isbn = {978-3-319-99977-7 978-3-319-99978-4},
  langid = {english},
  keywords = {/unread},
  annotation = {🏷️ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/J5QGPIIG/Wicht et al. - 2018 - DLL A Fast Deep Neural Network Library.pdf}
}

@article{jorda_performance_2019,
  title = {Performance {{Evaluation}} of {{cuDNN Convolution Algorithms}} on {{NVIDIA Volta GPUs}}},
  shorttitle = {{{cuDNN卷积算法在NVIDIA Volta GPU上的性能评估}}},
  author = {Jorda, Marc and {Valero-Lara}, Pedro and Pena, Antonio J.},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {70461--70473},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2918851},
  urldate = {2024-04-29},
  abstract = {Convolutional neural networks (CNNs) have recently attracted considerable attention due to their outstanding accuracy in applications, such as image recognition and natural language processing. While one advantage of the CNNs over other types of neural networks is their reduced computational cost, faster execution is still desired for both training and inference. Since convolution operations pose most of the execution time, multiple algorithms were and are being developed with the aim of accelerating this type of operations. However, due to the wide range of convolution parameter configurations used in the CNNs and the possible data type representations, it is not straightforward to assess in advance which of the available algorithms will be the best performing in each particular case. In this paper, we present a performance evaluation of the convolution algorithms provided by the cuDNN, the library used by most deep learning frameworks for their GPU operations. In our analysis, we leverage the convolution parameter configurations from widely used the CNNs and discuss which algorithms are better suited depending on the convolution parameters for both 32 and 16-bit floating-point (FP) data representations. Our results show that the filter size and the number of inputs are the most significant parameters when selecting a GPU convolution algorithm for 32-bit FP data. For 16-bit FP, leveraging specialized arithmetic units (NVIDIA Tensor Cores) is key to obtain the best performance. 【摘要翻译】由于卷积神经网络（CNN）在图像识别和自然语言处理等应用中的出色准确性，最近引起了相当大的关注。虽然CNN相对于其他类型神经网络的一个优势是它们降低了计算成本，但对于训练和推理来说，仍然需要更快的执行速度。由于卷积操作占据了大部分执行时间，因此过去和现在都在开发多种算法，目的是加速这种类型的操作。然而，由于CNN中使用的卷积参数配置范围广泛，以及可能的数据类型表示，提前评估哪些可用算法在每个特定情况下性能最佳并不简单。在本文中，我们对cuDNN提供的卷积算法进行了性能评估，cuDNN是大多数深度学习框架用于GPU操作的库。在我们的分析中，我们利用了广泛使用的CNN的卷积参数配置，并根据32位和16位浮点（FP）数据表示的卷积参数讨论了哪些算法更适合。我们的结果表明，在为32位FP数据选择GPU卷积算法时，过滤器大小和输入数量是最重要的参数。对于16位FP，利用专用算术单元（NVIDIA张量核心）是获得最佳性能的关键。},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  lccn = {3},
  keywords = {/unread},
  annotation = {🏷️ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/C6DU8DQG/Jorda et al. - 2019 - Performance Evaluation of cuDNN Convolution Algorithms on NVIDIA Volta GPUs.pdf}
}

@inproceedings{nazir_interpretable_2023,
  author = {Nazir, Zhumakhan and Yarovenko, Vladislav and Park, Jurn-Gyu},
  title = {Interpretable ML enhanced CNN Performance Analysis of cuBLAS, cuDNN and TensorRT},
  year = {2023},
  isbn = {9781450395175},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3555776.3578729},
  doi = {10.1145/3555776.3578729},
  abstract = {Deep learning models such as convolutional neural networks (CNNs) have a wide range of perception applications in image classification and object detection. However, despite the same CNN architectures, inference performance is different from implementations of specific libraries such as cuBLAS, cuDNN, and TensorRT. To investigate the performance effects of the state-of-the-art GPU libraries, this paper performs a case study of comparison and performance analysis of cuBLAS, cuDNN, and TensorRT implementations/libraries on YOLOv4-tiny, introducing crucial nvprof metrics for fair comparison and rationales of different performance and proposing interpretable machine learning (ML) model-based analysis. The results of our interpretable ML models show 100\% accuracy in the classification and 0.0094 MAPE in the regression tasks respectively.},
  booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
  pages = {1260–1265},
  numpages = {6},
  keywords = {interpretable ML, TensorRT, cuDNN, cuBLAS, GPU libraries, CNNs, deep learning},
  location = {Tallinn, Estonia},
  series = {SAC '23}
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    editor = "Goldstein, Jade  and
      Lavie, Alon  and
      Lin, Chin-Yew  and
      Voss, Clare",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}
