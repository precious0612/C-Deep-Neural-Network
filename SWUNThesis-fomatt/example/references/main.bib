% This file was created with JabRef 2.7.2.
% Encoding: UTF-8

% 1. æ–‡ç§‘ç±»å„æ•™å­¦å•ä½å¯¹è®¾è®¡ï¼ˆè®ºæ–‡ï¼‰çš„å¼•ã€æ³¨ã€å‚è€ƒæ–‡çŒ®çš„è‘—å½•è¦æ±‚ï¼Œç”±å„æ•™å­¦å•ä½æ ¹æ®å­¦ç§‘ç‰¹ç‚¹è‡ªè¡Œåˆ¶è®¢ç»Ÿä¸€ã€è§„èŒƒçš„å…·ä½“è¦æ±‚ï¼Œå¹¶æŠ¥æ•™åŠ¡å¤„å¤‡æ¡ˆã€‚
% 2. ç†å·¥å†œåŒ»å„ä¸“ä¸šæ¯•ä¸šå®ä¹ ã€æ¯•ä¸šè®¾è®¡ï¼ˆè®ºæ–‡ï¼‰å‚è€ƒæ–‡çŒ®è‘—å½•è¦æ±‚ï¼šå¼•ç”¨èµ„æ–™ã€æ–‡çŒ®ï¼Œå‡åº”è¯´æ˜æ¥æºã€‚è‘—å½•å¼•æ–‡çš„å‚è€ƒæ–‡çŒ®é‡‡ç”¨é¡ºåºç¼–ç åˆ¶ã€‚
% 3. é¡ºåºç¼–ç åˆ¶ï¼šæŒ‰æ–‡ç« æ­£æ–‡éƒ¨åˆ†ï¼ˆåŒ…æ‹¬å›¾ã€è¡¨åŠå…¶è¯´æ˜ï¼‰å¼•ç”¨æ–‡çŒ®çš„å…ˆåé¡ºåºè¿ç»­ç¼–ç ã€‚
%    ç¼–ç ç½®äºæ–¹æ‹¬å·ä¸­ï¼Œç”¨ä¸Šæ ‡çš„å½¢å¼ï¼ˆç½®äºå³ä¸Šè§’ï¼‰ï¼Œç›´æ¥æ”¾åœ¨å¼•æ–‡ä¹‹åï¼ˆå¦‚ï¼»1ï¼½ï¼›ï¼»15,18ï¼½ï¼›ï¼»25ï¼26ï¼½ï¼‰ã€‚

% ä¸»è¦è´£ä»»è€…è€….ä¹¦å.å…¶ä»–è´£ä»»è€….ç‰ˆæœ¬.å‡ºç‰ˆåœ°ï¼šå‡ºç‰ˆè€…,å‡ºç‰ˆå¹´ï¼šé¡µæ¬¡

@article{baldominos_survey_2019,
  title = {A {{Survey}} of {{Handwritten Character Recognition}} with {{MNIST}} and {{EMNIST}}},
  shorttitle = {{{MNISTå’ŒEMNISTæ‰‹å†™å­—ç¬¦è¯†åˆ«ç»¼è¿°}}},
  author = {Baldominos, Alejandro and Saez, Yago and Isasi, Pedro},
  year = {2019},
  month = aug,
  journal = {Applied Sciences},
  volume = {9},
  number = {15},
  pages = {3169},
  issn = {2076-3417},
  doi = {10.3390/app9153169},
  urldate = {2024-04-29},
  abstract = {This paper summarizes the top state-of-the-art contributions reported on the MNIST dataset for handwritten digit recognition. This dataset has been extensively used to validate novel techniques in computer vision, and in recent years, many authors have explored the performance of convolutional neural networks (CNNs) and other deep learning techniques over this dataset. To the best of our knowledge, this paper is the first exhaustive and updated review of this dataset; there are some online rankings, but they are outdated, and most published papers survey only closely related works, omitting most of the literature. This paper makes a distinction between those works using some kind of data augmentation and works using the original dataset out-of-the-box. Also, works using CNNs are reported separately; as they are becoming the state-of-the-art approach for solving this problem. Nowadays, a significant amount of works have attained a test error rate smaller than 1\% on this dataset; which is becoming non-challenging. By mid-2017, a new dataset was introduced: EMNIST, which involves both digits and letters, with a larger amount of data acquired from a database different than MNIST's. In this paper, EMNIST is explained and some results are surveyed. ã€æ‘˜è¦ç¿»è¯‘ã€‘æœ¬æ–‡æ€»ç»“äº†MNISTæ•°æ®é›†å¯¹æ‰‹å†™æ•°å­—è¯†åˆ«çš„æœ€å…ˆè¿›è´¡çŒ®ã€‚è¯¥æ•°æ®é›†å·²è¢«å¹¿æ³›ç”¨äºéªŒè¯è®¡ç®—æœºè§†è§‰ä¸­çš„æ–°æŠ€æœ¯ï¼Œè¿‘å¹´æ¥ï¼Œè®¸å¤šä½œè€…æ¢ç´¢äº†å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰å’Œå…¶ä»–æ·±åº¦å­¦ä¹ æŠ€æœ¯åœ¨è¯¥æ•°æ®é›†ä¸Šçš„æ€§èƒ½ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œæœ¬æ–‡æ˜¯å¯¹è¯¥æ•°æ®é›†çš„ç¬¬ä¸€æ¬¡è¯¦å°½å’Œæ›´æ–°çš„å›é¡¾ï¼›æœ‰ä¸€äº›åœ¨çº¿æ’åï¼Œä½†å®ƒä»¬å·²ç»è¿‡æ—¶ï¼Œå¤§å¤šæ•°å·²å‘è¡¨çš„è®ºæ–‡åªè°ƒæŸ¥å¯†åˆ‡ç›¸å…³çš„ä½œå“ï¼Œçœç•¥äº†å¤§éƒ¨åˆ†æ–‡çŒ®ã€‚æœ¬æ–‡åŒºåˆ†äº†é‚£äº›ä½¿ç”¨æŸç§æ•°æ®å¢å¼ºçš„ä½œå“å’Œä½¿ç”¨å¼€ç®±å³ç”¨åŸå§‹æ•°æ®é›†çš„ä½œå“ã€‚æ­¤å¤–ï¼Œä½¿ç”¨CNNçš„ä½œå“è¢«å•ç‹¬æŠ¥å‘Šï¼›å› ä¸ºå®ƒä»¬æ­£åœ¨æˆä¸ºè§£å†³è¿™ä¸ªé—®é¢˜çš„æœ€å…ˆè¿›æ–¹æ³•ã€‚å¦‚ä»Šï¼Œåœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šï¼Œå¤§é‡çš„å·¥ä½œå·²ç»è¾¾åˆ°äº†å°äº1\%çš„æµ‹è¯•é”™è¯¯ç‡ï¼›è¿™å˜å¾—æ²¡æœ‰æŒ‘æˆ˜æ€§ã€‚åˆ°2017å¹´å¹´ä¸­ï¼Œå¼•å…¥äº†ä¸€ä¸ªæ–°çš„æ•°æ®é›†ï¼šEMNISTï¼Œå®ƒæ¶‰åŠæ•°å­—å’Œå­—æ¯ï¼Œä»ä¸åŒäºMNISTçš„æ•°æ®åº“ä¸­è·å–äº†å¤§é‡æ•°æ®ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œå¯¹EMNISTè¿›è¡Œäº†è§£é‡Šï¼Œå¹¶å¯¹ä¸€äº›ç»“æœè¿›è¡Œäº†è°ƒæŸ¥ã€‚},
  copyright = {https://creativecommons.org/licenses/by/4.0/},
  langid = {english},
  lccn = {4},
  keywords = {/unread},
  annotation = {ğŸ·ï¸ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/9BAD4LZJ/Baldominos et al. - 2019 - A Survey of Handwritten Character Recognition with MNIST and EMNIST.pdf}
}

@misc{google_tensorflow_2023,
  title = {Tensorflow {{C API}}},
  author = {Google},
  year = {2023},
  month = nov,
  keywords = {/unread},
  annotation = {ğŸ·ï¸ /unread}
}

@article{ho_denoising_nodate,
  title = {Denoising {{Diffusion Probabilistic Models}}},
  shorttitle = {å»å™ªæ‰©æ•£æ¦‚ç‡æ¨¡å‹},
  author = {Ho, Jonathan and Jain, Ajay and Abbeel, Pieter},
  journal = {arXiv},
  abstract = {We present high quality image synthesis results using diffusion probabilistic models, a class of latent variable models inspired by considerations from nonequilibrium thermodynamics. Our best results are obtained by training on a weighted variational bound designed according to a novel connection between diffusion probabilistic models and denoising score matching with Langevin dynamics, and our models naturally admit a progressive lossy decompression scheme that can be interpreted as a generalization of autoregressive decoding. On the unconditional CIFAR10 dataset, we obtain an Inception score of 9.46 and a state-of-the-art FID score of 3.17. On 256x256 LSUN, we obtain sample quality similar to ProgressiveGAN. Our implementation is available at https://github.com/hojonathanho/diffusion. ã€æ‘˜è¦ç¿»è¯‘ã€‘æˆ‘ä»¬ä½¿ç”¨æ‰©æ•£æ¦‚ç‡æ¨¡å‹å‘ˆç°é«˜è´¨é‡çš„å›¾åƒåˆæˆç»“æœï¼Œè¿™æ˜¯ä¸€ç±»å—éå¹³è¡¡çƒ­åŠ›å­¦è€ƒè™‘å¯å‘çš„æ½œåœ¨å˜é‡æ¨¡å‹ã€‚æˆ‘ä»¬çš„æœ€ä½³ç»“æœæ˜¯é€šè¿‡åœ¨åŠ æƒå˜åˆ†ç•Œä¸Šè®­ç»ƒè·å¾—çš„ï¼Œè¯¥å˜åˆ†ç•Œæ˜¯æ ¹æ®æ‰©æ•£æ¦‚ç‡æ¨¡å‹å’Œä¸æœ—ä¹‹ä¸‡åŠ¨åŠ›å­¦çš„å»å™ªåˆ†æ•°åŒ¹é…ä¹‹é—´çš„æ–°é¢–è”ç³»è€Œè®¾è®¡çš„ï¼Œæˆ‘ä»¬çš„æ¨¡å‹è‡ªç„¶æ‰¿è®¤ä¸€ç§æ¸è¿›çš„æœ‰æŸè§£å‹ç¼©æ–¹æ¡ˆï¼Œå¯ä»¥è§£é‡Šä¸ºè‡ªå›å½’è§£ç çš„æ³›åŒ–ã€‚åœ¨æ— æ¡ä»¶çš„CIFAR10æ•°æ®é›†ä¸Šï¼Œæˆ‘ä»¬è·å¾—äº†9.46çš„åˆå§‹åˆ†æ•°å’Œ3.17çš„æœ€å…ˆè¿›çš„FIDåˆ†æ•°ã€‚åœ¨256x256 LSUNä¸Šï¼Œæˆ‘ä»¬è·å¾—äº†ç±»ä¼¼äºProgressiveGANçš„æ ·æœ¬è´¨é‡ã€‚æˆ‘ä»¬çš„å®ç°å¯åœ¨https://github.com/hojonathanho/diffusionè·å¾—ã€‚},
  langid = {english},
  keywords = {/unread,diffusion model},
  annotation = {ğŸ·ï¸ /unreadã€diffusion model},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/9PHVM6HW/Ho et al. - Denoising Diffusion Probabilistic Models.pdf}
}

@misc{li_studying_2022,
  title = {Studying {{Popular Open Source Machine Learning Libraries}} and {{Their Cross-Ecosystem Bindings}}},
  shorttitle = {ç ”ç©¶æµè¡Œçš„å¼€æºæœºå™¨å­¦ä¹ åº“åŠå…¶è·¨ç”Ÿæ€ç³»ç»Ÿç»‘å®š},
  author = {Li, Hao and Bezemer, Cor-Paul},
  year = {2022},
  month = jan,
  number = {arXiv:2201.07201},
  eprint = {2201.07201},
  publisher = {arXiv},
  urldate = {2024-01-07},
  abstract = {Open source machine learning (ML) libraries allow developers to integrate advanced ML functionality into their own applications. However, popular ML libraries, such as TensorFlow, are not available natively in all programming languages and software package ecosystems. Hence, developers who wish to use an ML library which is not available in their programming language or ecosystem of choice, may need to resort to using a so-called binding library. Binding libraries provide support across programming languages and package ecosystems for a source library. For example, the Keras .NET binding provides support for the Keras library in the NuGet (.NET) ecosystem even though the Keras library was written in Python. In this paper, we conduct an in-depth study of 155 cross-ecosystem bindings and their development for 36 popular open source ML libraries. Our study shows that for most popular ML libraries, only one package ecosystem is officially supported (usually PyPI). Cross-ecosystem support, which is available for 25\% of the studied ML libraries, is usually provided through community-maintained bindings, e.g., 73\% of the bindings in the npm ecosystem are community-maintained. Our study shows that the vast majority of the studied bindings cover only a small portion of the source library releases, and the delay for receiving support for a source library release is large. ã€æ‘˜è¦ç¿»è¯‘ã€‘å¼€æºæœºå™¨å­¦ä¹ ï¼ˆMLï¼‰åº“å…è®¸å¼€å‘äººå‘˜å°†é«˜çº§MLåŠŸèƒ½é›†æˆåˆ°è‡ªå·±çš„åº”ç”¨ç¨‹åºä¸­ã€‚ä½†æ˜¯ï¼Œæµè¡Œçš„MLåº“ï¼ˆä¾‹å¦‚TensorFlowï¼‰åœ¨æ‰€æœ‰ç¼–ç¨‹è¯­è¨€å’Œè½¯ä»¶åŒ…ç”Ÿæ€ç³»ç»Ÿä¸­éƒ½ä¸å¯ç”¨ã€‚å› æ­¤ï¼Œå¸Œæœ›ä½¿ç”¨MLåº“çš„å¼€å‘äººå‘˜å¯èƒ½éœ€è¦è¯‰è¯¸äºä½¿ç”¨æ‰€è°“çš„ç»‘å®šåº“ã€‚ç»‘å®šåº“ä¸ºæºåº“æä¾›äº†è·¨ç¼–ç¨‹è¯­è¨€å’ŒåŒ…è£…ç”Ÿæ€ç³»ç»Ÿçš„æ”¯æŒã€‚ä¾‹å¦‚ï¼Œå³ä½¿KERASåº“æ˜¯ç”¨Pythonç¼–å†™çš„ï¼ŒKeras .NETç»‘å®šä¹Ÿä¸ºNugetï¼ˆ.NETï¼‰ç”Ÿæ€ç³»ç»Ÿä¸­çš„Kerasåº“æä¾›äº†æ”¯æŒã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹36ä¸ªæµè¡Œçš„å¼€æºMLåº“è¿›è¡Œäº†155ä¸ªè·¨ç”Ÿæ€ç³»ç»Ÿç»‘å®šåŠå…¶å¼€å‘çš„æ·±å…¥ç ”ç©¶ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œå¯¹äºæœ€å—æ¬¢è¿çš„MLåº“ï¼Œåªæœ‰ä¸€ä¸ªåŒ…è£…ç”Ÿæ€ç³»ç»Ÿå—åˆ°äº†æ”¯æŒï¼ˆé€šå¸¸æ˜¯PYPIï¼‰ã€‚é€šå¸¸é€šè¿‡ç¤¾åŒºç»´æŠ¤çš„ç»‘å®šæä¾›çš„è·¨ç”Ÿæ€ç³»ç»Ÿæ”¯æŒï¼Œå¯ç”¨äºç ”ç©¶çš„MLåº“ä¸­25ï¼…ï¼Œä¾‹å¦‚ï¼ŒNPMç”Ÿæ€ç³»ç»Ÿä¸­73ï¼…çš„ç»‘å®šæ˜¯ç¤¾åŒºç»´æŠ¤çš„ã€‚æˆ‘ä»¬çš„ç ”ç©¶è¡¨æ˜ï¼Œç»å¤§å¤šæ•°ç ”ç©¶çš„ç»‘å®šä»…æ¶µç›–æºåº“çš„ä¸€å°éƒ¨åˆ†ï¼Œå¹¶ä¸”æ¥æ”¶å¯¹æºåº“é‡Šæ”¾çš„æ”¯æŒçš„å»¶è¿Ÿå¾ˆå¤§ã€‚},
  archiveprefix = {arxiv},
  langid = {english},
  keywords = {/unread,Computer Science - Machine Learning,Computer Science - Software Engineering},
  annotation = {ğŸ·ï¸ /unreadã€Computer Science - Machine Learningã€Computer Science - Software Engineering},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/VA8S4TAW/Li and Bezemer - 2022 - Studying Popular Open Source Machine Learning Libraries and Their Cross-Ecosystem Bindings.pdf}
}

@inproceedings{macukow_neural_2016,
  title = {Neural Networks -- State of Art, Brief History, Basic Models and Architecture},
  shorttitle = {ç¥ç»ç½‘ç»œ-æœ€å…ˆè¿›çš„æŠ€æœ¯ã€ç®€å²ã€åŸºæœ¬æ¨¡å‹å’Œæ¶æ„},
  booktitle = {Computer Information Systems and Industrial Management},
  author = {Macukow, Bohdan},
  editor = {Saeed, Khalid and Homenda, W{\l}adys{\l}aw},
  year = {2016},
  pages = {3--14},
  publisher = {Springer International Publishing},
  address = {Cham},
  abstract = {The history of neural networks can be traced back to the work of trying to model the neuron. Today, neural networks discussions are occurring everywhere. Neural networks, with their remarkable ability to derive meaning from complicated or imprecise data, can be used to extract patterns and detect trends that are too complex to be noticed by either humans or other computer techniques. A brief history of the neural networks research is presented and some more popular models are briefly discussed. The major attention is on the feed-forward networks and specially to the topology of such the network and method of building the multi-layer perceptrons. ã€æ‘˜è¦ç¿»è¯‘ã€‘ç¥ç»ç½‘ç»œçš„å†å²å¯ä»¥è¿½æº¯åˆ°è¯•å›¾æ¨¡æ‹Ÿç¥ç»å…ƒçš„å·¥ä½œã€‚ä»Šå¤©ï¼Œç¥ç»ç½‘ç»œçš„è®¨è®ºæ— å¤„ä¸åœ¨ã€‚ç¥ç»ç½‘ç»œå…·æœ‰ä»å¤æ‚æˆ–ä¸ç²¾ç¡®çš„æ•°æ®ä¸­è·å¾—æ„ä¹‰çš„éå‡¡èƒ½åŠ›ï¼Œå¯ç”¨äºæå–æ¨¡å¼å’Œæ£€æµ‹è¿‡äºå¤æ‚è€Œæ— æ³•è¢«äººç±»æˆ–å…¶ä»–è®¡ç®—æœºæŠ€æœ¯æ³¨æ„åˆ°çš„è¶‹åŠ¿ã€‚ä»‹ç»äº†ç¥ç»ç½‘ç»œç ”ç©¶çš„ç®€å²ï¼Œå¹¶ç®€è¦è®¨è®ºäº†ä¸€äº›æ›´æµè¡Œçš„æ¨¡å‹ã€‚ä¸»è¦å…³æ³¨çš„æ˜¯å‰é¦ˆç½‘ç»œï¼Œç‰¹åˆ«æ˜¯è¿™ç§ç½‘ç»œçš„æ‹“æ‰‘å’Œæ„å»ºå¤šå±‚æ„ŸçŸ¥å™¨çš„æ–¹æ³•ã€‚},
  isbn = {978-3-319-45378-1},
  langid = {english},
  keywords = {/unread},
  annotation = {ğŸ·ï¸ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/E53X8XL2/978-3-319-45378-1_1.pdf}
}

@misc{pytorch_pytorch_2022,
  title = {{{PYTORCH C}}++ {{API}}},
  author = {PyTorch, Contributors},
  year = {2022},
  publisher = {Pytorch},
  copyright = {{\copyright} Copyright 2022, PyTorch Contributors},
  langid = {english},
  keywords = {/unread},
  annotation = {ğŸ·ï¸ /unread}
}

@article{saha_machine_2022,
  title = {Machine {{Learning}} for {{Microcontroller-Class Hardware}}: {{A Review}}},
  shorttitle = {å¾®æ§åˆ¶å™¨çº§ç¡¬ä»¶çš„æœºå™¨å­¦ä¹ ï¼šè¯„è®º},
  author = {Saha, Swapnil Sayan and Sandha, Sandeep Singh and Srivastava, Mani},
  year = {2022},
  month = nov,
  journal = {IEEE Sensors Journal},
  volume = {22},
  number = {22},
  pages = {21362--21390},
  issn = {1530-437X},
  doi = {10.1109/JSEN.2022.3210773},
  urldate = {2024-01-07},
  abstract = {The advancements in machine learning (ML) opened a new opportunity to bring intelligence to the low-end Internet-of-Things (IoT) nodes, such as microcontrollers. Conventional ML deployment has high memory and computes footprint hindering their direct deployment on ultraresource-constrained microcontrollers. This article highlights the unique requirements of enabling onboard ML for microcontroller-class devices. Researchers use a specialized model development workflow for resource-limited applications to ensure that the compute and latency budget is within the device limits while still maintaining the desired performance. We characterize a closed-loop widely applicable workflow of ML model development for microcontroller-class devices and show that several classes of applications adopt a specific instance of it. We present both qualitative and numerical insights into different stages of model development by showcasing several use cases. Finally, we identify the open research challenges and unsolved questions demanding careful considerations moving forward. ã€æ‘˜è¦ç¿»è¯‘ã€‘æœºå™¨å­¦ä¹ çš„è¿›æ­¥ï¼ˆMLï¼‰ä¸ºä½ç«¯äº’è”ç½‘ï¼ˆIoTï¼‰èŠ‚ç‚¹ï¼ˆä¾‹å¦‚å¾®æ§åˆ¶å™¨ï¼‰å¸¦æ¥äº†æ–°çš„æœºä¼šï¼Œå°†æƒ…æŠ¥å¸¦å…¥äº†æ™ºèƒ½ã€‚ä¼ ç»Ÿçš„MLéƒ¨ç½²å…·æœ‰å¾ˆé«˜çš„è®°å¿†åŠ›ï¼Œå¹¶è®¡ç®—å åœ°é¢ç§¯ï¼Œé˜»ç¢äº†å…¶åœ¨è¶…å¤§é™åˆ¶çš„å¾®æ§åˆ¶å™¨ä¸Šçš„ç›´æ¥éƒ¨ç½²ã€‚æœ¬æ–‡é‡ç‚¹ä»‹ç»äº†ä¸ºå¾®æ§åˆ¶å™¨çº§è®¾å¤‡å¯ç”¨æ¿è½½MLçš„ç‹¬ç‰¹è¦æ±‚ã€‚ç ”ç©¶äººå‘˜ä½¿ç”¨ä¸“é—¨çš„æ¨¡å‹å¼€å‘å·¥ä½œæµè¿›è¡Œèµ„æºæœ‰é™çš„åº”ç”¨ç¨‹åºï¼Œä»¥ç¡®ä¿è®¡ç®—å’Œå»¶è¿Ÿé¢„ç®—åœ¨è®¾å¤‡é™åˆ¶ä¹‹å†…ï¼ŒåŒæ—¶ä»ä¿æŒæ‰€éœ€çš„æ€§èƒ½ã€‚æˆ‘ä»¬è¡¨å¾äº†å¾®æ§åˆ¶å™¨çº§è®¾å¤‡çš„MLæ¨¡å‹å¼€å‘çš„å¹¿æ³›é€‚ç”¨çš„é—­ç¯å·¥ä½œæµï¼Œå¹¶è¡¨æ˜å‡ ç±»åº”ç”¨ç¨‹åºé‡‡ç”¨äº†å®ƒçš„ç‰¹å®šå®ä¾‹ã€‚æˆ‘ä»¬é€šè¿‡å±•ç¤ºå¤šç§ç”¨ä¾‹ï¼Œå‘æ¨¡å‹å¼€å‘çš„ä¸åŒé˜¶æ®µæä¾›å®šæ€§å’Œæ•°å€¼è§è§£ã€‚æœ€åï¼Œæˆ‘ä»¬ç¡®å®šäº†å¼€æ”¾çš„ç ”ç©¶æŒ‘æˆ˜å’Œæœªè§£å†³çš„é—®é¢˜ï¼Œè¦æ±‚ä»”ç»†è€ƒè™‘å‰è¿›ã€‚},
  langid = {english},
  lccn = {2},
  keywords = {/unread},
  annotation = {ğŸ·ï¸ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/ESFJ9MBU/Saha et al. - 2022 - Machine Learning for Microcontroller-Class Hardware A Review.pdf}
}

@article{sakr_machine_2020,
  title = {Machine {{Learning}} on {{Mainstream Microcontrollers}}},
  shorttitle = {ä¸»æµå¾®æ§åˆ¶å™¨ä¸Šçš„æœºå™¨å­¦ä¹ },
  author = {Sakr, Fouad and Bellotti, Francesco and Berta, Riccardo and De Gloria, Alessandro},
  year = {2020},
  month = may,
  journal = {Sensors},
  volume = {20},
  number = {9},
  pages = {2638},
  issn = {1424-8220},
  doi = {10.3390/s20092638},
  urldate = {2024-01-07},
  abstract = {This paper presents the Edge Learning Machine (ELM), a machine learning framework for edge devices, which manages the training phase on a desktop computer and performs inferences on microcontrollers. The framework implements, in a platform-independent C language, three supervised machine learning algorithms (Support Vector Machine (SVM) with a linear kernel, k-Nearest Neighbors (K-NN), and Decision Tree (DT)), and exploits STM X-Cube-AI to implement Artificial Neural Networks (ANNs) on STM32 Nucleo boards. We investigated the performance of these algorithms on six embedded boards and six datasets (four classifications and two regression). Our analysis---which aims to plug a gap in the literature---shows that the target platforms allow us to achieve the same performance score as a desktop machine, with a similar time latency. ANN performs better than the other algorithms in most cases, with no difference among the target devices. We observed that increasing the depth of an NN improves performance, up to a saturation level. k-NN performs similarly to ANN and, in one case, even better, but requires all the training sets to be kept in the inference phase, posing a significant memory demand, which can be afforded only by high-end edge devices. DT performance has a larger variance across datasets. In general, several factors impact performance in different ways across datasets. This highlights the importance of a framework like ELM, which is able to train and compare different algorithms. To support the developer community, ELM is released on an open-source basis. ã€æ‘˜è¦ç¿»è¯‘ã€‘æœ¬æ–‡ä»‹ç»äº†Edge Learning Machineï¼ˆELMï¼‰ï¼Œè¿™æ˜¯ä¸€ä¸ªç”¨äºEdgeè®¾å¤‡çš„æœºå™¨å­¦ä¹ æ¡†æ¶ï¼Œè¯¥æ¡†æ¶åœ¨å°å¼è®¡ç®—æœºä¸Šç®¡ç†è®­ç»ƒé˜¶æ®µï¼Œå¹¶åœ¨å¾®æ§åˆ¶å™¨ä¸Šæ‰§è¡Œæ¨æ–­ã€‚è¯¥æ¡†æ¶ä»¥ç‹¬ç«‹äºå¹³å°çš„Cè¯­è¨€å®ç°äº†ä¸‰ç§æœ‰ç›‘ç£çš„æœºå™¨å­¦ä¹ ç®—æ³•ï¼ˆæ”¯æŒå‘é‡æœºï¼ˆSVMï¼‰ï¼Œå¸¦æœ‰çº¿æ€§å†…æ ¸ï¼ŒK-Nearesté‚»å±…ï¼ˆK-NNï¼‰å’Œå†³ç­–æ ‘ï¼ˆDTï¼‰ï¼‰ï¼Œå¹¶åˆ©ç”¨STM X-Cube-AIåœ¨STM32æ ¸æ¿ä¸Šå®æ–½äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰ã€‚æˆ‘ä»¬ç ”ç©¶äº†è¿™äº›ç®—æ³•åœ¨å…­ä¸ªåµŒå…¥å¼æ¿å’Œå…­ä¸ªæ•°æ®é›†ä¸Šçš„æ€§èƒ½ï¼ˆå››ä¸ªåˆ†ç±»å’Œä¸¤ä¸ªå›å½’ï¼‰ã€‚æˆ‘ä»¬çš„åˆ†æï¼ˆæ—¨åœ¨ç¼©å°æ–‡çŒ®ä¸­çš„å·®è·ï¼‰å±•ç¤ºäº†ç›®æ ‡å¹³å°ä½¿æˆ‘ä»¬èƒ½å¤Ÿè·å¾—ä¸å°å¼æœºç›¸åŒçš„æ€§èƒ½å¾—åˆ†ï¼Œå¹¶ä¸”å…·æœ‰ç›¸ä¼¼çš„æ—¶é—´å»¶è¿Ÿã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼ŒANNçš„æ€§èƒ½è¦æ¯”å…¶ä»–ç®—æ³•æ›´å¥½ï¼Œè€Œç›®æ ‡è®¾å¤‡ä¹‹é—´æ²¡æœ‰åŒºåˆ«ã€‚æˆ‘ä»¬è§‚å¯Ÿåˆ°ï¼Œå¢åŠ NNçš„æ·±åº¦å¯æé«˜æ€§èƒ½ï¼Œæœ€é«˜é¥±å’Œæ°´å¹³ã€‚ K-NNçš„æ€§èƒ½ä¸ANNç›¸ä¼¼ï¼Œåœ¨ä¸€ç§æƒ…å†µä¸‹ç”šè‡³æ›´å¥½ï¼Œä½†è¦æ±‚å°†æ‰€æœ‰è®­ç»ƒé›†ä¿å­˜åœ¨æ¨ç†é˜¶æ®µï¼Œå¹¶æå‡ºæ˜æ˜¾çš„å†…å­˜éœ€æ±‚ï¼Œåªèƒ½ç”±é«˜ç«¯è¾¹ç¼˜è®¾å¤‡é™åˆ¶ã€‚ DTæ€§èƒ½åœ¨æ•°æ®é›†ä¹‹é—´å…·æœ‰è¾ƒå¤§çš„å·®å¼‚ã€‚é€šå¸¸ï¼Œå‡ ä¸ªå› ç´ ä¼šå½±å“æ•´ä¸ªæ•°æ®é›†ä¸åŒæ–¹å¼çš„æ€§èƒ½ã€‚è¿™çªå‡ºäº†åƒELMè¿™æ ·çš„æ¡†æ¶çš„é‡è¦æ€§ï¼Œè¯¥æ¡†æ¶èƒ½å¤Ÿè®­ç»ƒå’Œæ¯”è¾ƒä¸åŒçš„ç®—æ³•ã€‚ä¸ºäº†æ”¯æŒå¼€å‘äººå‘˜ç¤¾åŒºï¼ŒELMæ˜¯å¼€æºçš„ã€‚},
  langid = {english},
  lccn = {3},
  keywords = {/unread},
  annotation = {ğŸ·ï¸ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/7EQ2EU4D/Sakr et al. - 2020 - Machine Learning on Mainstream Microcontrollers.pdf}
}

@article{sullivan_structure_nodate,
  author = {Sullivan, Kevin J. and Griswold, William G. and Cai, Yuanfang and Hallen, Ben},
  title = {The structure and value of modularity in software design},
  year = {2001},
  issue_date = {Sept. 2001},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {26},
  number = {5},
  issn = {0163-5948},
  url = {https://doi.org/10.1145/503271.503224},
  doi = {10.1145/503271.503224},
  abstract = {The concept of information hiding modularity is a cornerstone of modern software design thought, but its formulation remains casual and its emphasis on changeability is imperfectly related to the goal of creating added value in a given context. We need better explanatory and prescriptive models of the nature and value of information hiding. We evaluate the potential of a new theory---developed to account for the influence of modularity on the evolution of the computer industry---to inform software design. The theory uses design structure matrices to model designs and real options techniques to value them. To test the potential utility of the theory for software we apply it to Parnas's KWIC designs. We contribute an extension to design structure matrices, and we show that the options results are consistent with Parnas's conclusions. Our results suggest that such a theory does have potential to help inform software design.},
  journal = {SIGSOFT Softw. Eng. Notes},
  month = {sep},
  pages = {99â€“108},
  numpages = {10},
  keywords = {software, real options, modularity, design structure matrix}
}

@inproceedings{tempero_experiment_2023,
  author = {Tempero, Ewan and Blincoe, Kelly and Lottridge, Danielle},
  title = {An Experiment on the Effects of Modularity on Code Modification and Understanding},
  year = {2023},
  isbn = {9781450399418},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3576123.3576138},
  doi = {10.1145/3576123.3576138},
  abstract = {Good modularity is seen as an important goal in software design. Achieving this goal is claimed to improve, among other things, the understandability and modifiability of a design. Yet, when teaching software design, we see that students limit the amount of modularity that they introduce into their code and cannot see the benefit of further modularity. This could be because they do not understand the benefits, but it could also be that these benefits are limited for inexperienced developers. In order to teach the benefits of modularity we need to understand what, if any, benefits exist for students. We conducted a controlled experiment where 40 students performed a modification task on two different designs, one with higher modularity than the other. Students were better able to successfully complete the task with the design with higher modularity. However, we found a trend where understanding was lower for the high modularity design. These results suggest modularity is beneficial to students, and that understanding of modularity needs to be better supported when teaching software design.},
  booktitle = {Proceedings of the 25th Australasian Computing Education Conference},
  pages = {105â€“112},
  numpages = {8},
  keywords = {code understandability, empirical software engineering, modifiability, modularity},
  location = {<conf-loc>, <city>Melbourne</city>, <state>VIC</state>, <country>Australia</country>, </conf-loc>},
  series = {ACE '23}
}

@article{arkhangelskaya_deep_2023,
  title = {Deep {{Learning}} for {{Natural Language Processing}}: {{A Survey}}},
  shorttitle = {è‡ªç„¶è¯­è¨€å¤„ç†çš„æ·±åº¦å­¦ä¹ },
  author = {Arkhangelskaya, E. O. and Nikolenko, S. I.},
  year = {2023},
  month = jul,
  journal = {Journal of Mathematical Sciences},
  volume = {273},
  number = {4},
  pages = {533--582},
  issn = {1072-3374},
  doi = {10.1007/s10958-023-06519-6},
  urldate = {2024-04-29},
  langid = {english},
  keywords = {/unread},
  annotation = {ğŸ·ï¸ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/2968AV86/Arkhangelskaya and Nikolenko - 2023 - Deep Learning for Natural Language Processing A Survey.pdf}
}

@article{akopov_modeling_2023,
  title = {Modeling and Optimization of Strategies for Making Individual Decisions in Multi-Agent Socio-Economic Systems with the Use of Machine Learning},
  shorttitle = {ä½¿ç”¨æœºå™¨å­¦ä¹ å¯¹å¤šæ™ºèƒ½ä½“ç¤¾ä¼šç»æµç³»ç»Ÿä¸­çš„ä¸ªäººå†³ç­–ç­–ç•¥è¿›è¡Œå»ºæ¨¡å’Œä¼˜åŒ–},
  author = {Akopov, Andranik},
  year = {2023},
  month = jun,
  journal = {Business Informatics},
  volume = {17},
  number = {2},
  pages = {7--19},
  issn = {2587-814X},
  doi = {10.17323/2587-814X.2023.2.7.19},
  urldate = {2024-04-29},
  abstract = {This article presents a new approach to modeling and optimizing individual decision-making strategies in multi-agent socio-economic systems (MSES). This approach is based on the synthesis of agent-based modeling methods, machine learning and genetic optimization algorithms. A procedure for the synthesis and training of artificial neural networks (ANNs) that simulate the functionality of MSES and provide an approximation of the values of its objective characteristics has been developed. The feature of the two-step procedure is the combined use of particle swarm optimization methods (to determine the optimal values of hyperparameters) and the Adam machine learning algorithm (to compute weight coefficients of the ANN). The use of such ANN-based surrogate models in parallel multi-agent real-coded genetic algorithms (MA-RCGA) makes it possible to raise substantially the time-efficiency of the evolutionary search for optimal solutions. We have conducted numerical experiments that confirm a significant improvement in the performance of MA-RCGA, which periodically uses the ANN-based surrogate-model to approximate the values of the objective and fitness functions. A software framework has been designed that consists of the original (reference) agent-based model of trade interactions, the ANN-based surrogate model and the MA-RCGA genetic algorithm. At the same time, the software libraries FLAME GPU, OpenNN (Open Neural Networks Library), etc., agent-based modeling and machine learning methods are used. The system we developed can be used by responsible managers. ã€æ‘˜è¦ç¿»è¯‘ã€‘æœ¬æ–‡æå‡ºäº†ä¸€ç§åœ¨å¤šæ™ºèƒ½ä½“ç¤¾ä¼šç»æµç³»ç»Ÿï¼ˆMSESï¼‰ä¸­å»ºæ¨¡å’Œä¼˜åŒ–ä¸ªäººå†³ç­–ç­–ç•¥çš„æ–°æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•åŸºäºåŸºäºæ™ºèƒ½ä½“çš„å»ºæ¨¡æ–¹æ³•ã€æœºå™¨å­¦ä¹ å’Œé—ä¼ ä¼˜åŒ–ç®—æ³•çš„ç»¼åˆã€‚å·²ç»å¼€å‘äº†ä¸€ä¸ªæ¨¡æ‹ŸMSESåŠŸèƒ½å¹¶æä¾›å…¶å®¢è§‚ç‰¹å¾å€¼è¿‘ä¼¼å€¼çš„äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰çš„ç»¼åˆå’Œè®­ç»ƒç¨‹åºã€‚ä¸¤æ­¥ç¨‹åºçš„ç‰¹ç‚¹æ˜¯ç»“åˆä½¿ç”¨ç²’å­ç¾¤ä¼˜åŒ–æ–¹æ³•ï¼ˆç¡®å®šè¶…å‚æ•°çš„æœ€ä¼˜å€¼ï¼‰å’Œäºšå½“æœºå™¨å­¦ä¹ ç®—æ³•ï¼ˆè®¡ç®—ANNçš„æƒé‡ç³»æ•°ï¼‰ã€‚åœ¨å¹¶è¡Œå¤šæ™ºèƒ½ä½“å®æ•°ç¼–ç é—ä¼ ç®—æ³•ï¼ˆMA-RCGAï¼‰ä¸­ä½¿ç”¨è¿™ç§åŸºäºANNçš„ä»£ç†æ¨¡å‹ä½¿å¾—æœ‰å¯èƒ½å¤§å¤§æé«˜è¿›åŒ–æœç´¢æœ€ä¼˜è§£çš„æ—¶é—´æ•ˆç‡ã€‚æˆ‘ä»¬è¿›è¡Œäº†æ•°å€¼å®éªŒï¼Œè¯å®äº†MA-RCGAæ€§èƒ½çš„æ˜¾è‘—æé«˜ï¼Œè¯¥ç®—æ³•å‘¨æœŸæ€§åœ°ä½¿ç”¨åŸºäºANNçš„ä»£ç†æ¨¡å‹æ¥è¿‘ä¼¼ç›®æ ‡å’Œé€‚åº”åº¦å‡½æ•°çš„å€¼ã€‚è®¾è®¡äº†ä¸€ä¸ªè½¯ä»¶æ¡†æ¶ï¼Œç”±åŸå§‹çš„ï¼ˆå‚è€ƒï¼‰åŸºäºä»£ç†çš„è´¸æ˜“äº¤äº’æ¨¡å‹ã€åŸºäºANNçš„ä»£ç†æ¨¡å‹å’ŒMA-RCGAé—ä¼ ç®—æ³•ç»„æˆã€‚åŒæ—¶ï¼Œä½¿ç”¨äº†è½¯ä»¶åº“FLAME GPUã€OpenNNï¼ˆå¼€æ”¾ç¥ç»ç½‘ç»œåº“ï¼‰ç­‰ï¼ŒåŸºäºä»£ç†çš„å»ºæ¨¡å’Œæœºå™¨å­¦ä¹ æ–¹æ³•ã€‚æˆ‘ä»¬å¼€å‘çš„ç³»ç»Ÿå¯ä»¥ä¾›è´Ÿè´£çš„ç®¡ç†è€…ä½¿ç”¨ã€‚},
  langid = {english},
  keywords = {/unread},
  annotation = {ğŸ·ï¸ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/YGV6AJNA/Akopov - 2023 - Modeling and optimization of strategies for making individual decisions in multi-agent socio-economi.pdf}
}

@incollection{pancioni_dll_2018,
  title = {{{DLL}}: {{A Fast Deep Neural Network Library}}},
  shorttitle = {{{DLL}}ï¼šä¸€ä¸ªå¿«é€Ÿçš„æ·±åº¦ç¥ç»ç½‘ç»œåº“},
  booktitle = {Artificial {{Neural Networks}} in {{Pattern Recognition}}},
  author = {Wicht, Baptiste and Fischer, Andreas and Hennebert, Jean},
  editor = {Pancioni, Luca and Schwenker, Friedhelm and Trentin, Edmondo},
  year = {2018},
  volume = {11081},
  pages = {54--65},
  publisher = {Springer International Publishing},
  address = {Cham},
  urldate = {2024-04-29},
  abstract = {Deep Learning Library (DLL) is a library for machine learning with deep neural networks that focuses on speed. It supports feedforward neural networks such as fully-connected Artificial Neural Networks (ANNs) and Convolutional Neural Networks (CNNs). Our main motivation for this work was to propose and evaluate novel software engineering strategies with potential to accelerate runtime for training and inference`. Such strategies are mostly independent of the underlying deep learning algorithms. On three different datasets and for four different neural network models, we compared DLL to five popular deep learning libraries. Experimentally, it is shown that the proposed library is systematically and significantly faster on CPU and GPU. In terms of classification performance, similar accuracies as the other libraries are reported. ã€æ‘˜è¦ç¿»è¯‘ã€‘æ·±åº¦å­¦ä¹ åº“ï¼ˆDLLï¼‰æ˜¯ä¸€ä¸ªä¸“æ³¨äºé€Ÿåº¦çš„æ·±åº¦ç¥ç»ç½‘ç»œæœºå™¨å­¦ä¹ åº“ã€‚å®ƒæ”¯æŒå‰é¦ˆç¥ç»ç½‘ç»œï¼Œå¦‚å…¨è¿æ¥çš„äººå·¥ç¥ç»ç½‘ç»œï¼ˆANNï¼‰å’Œå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ã€‚æˆ‘ä»¬è¿™é¡¹å·¥ä½œçš„ä¸»è¦åŠ¨æœºæ˜¯æå‡ºå’Œè¯„ä¼°æ–°çš„è½¯ä»¶å·¥ç¨‹ç­–ç•¥ï¼Œè¿™äº›ç­–ç•¥æœ‰å¯èƒ½åŠ é€Ÿè®­ç»ƒå’Œæ¨ç†çš„è¿è¡Œæ—¶é—´ã€‚è¿™äº›ç­–ç•¥å¤§å¤šç‹¬ç«‹äºåº•å±‚æ·±åº¦å­¦ä¹ ç®—æ³•ã€‚åœ¨ä¸‰ä¸ªä¸åŒçš„æ•°æ®é›†å’Œå››ä¸ªä¸åŒçš„ç¥ç»ç½‘ç»œæ¨¡å‹ä¸Šï¼Œæˆ‘ä»¬å°†DLLä¸äº”ä¸ªæµè¡Œçš„æ·±åº¦å­¦ä¹ åº“è¿›è¡Œäº†æ¯”è¾ƒã€‚å®éªŒè¡¨æ˜ï¼Œæ‰€æå‡ºçš„åº“åœ¨CPUå’ŒGPUä¸Šç³»ç»Ÿä¸”æ˜¾ç€åœ°æ›´å¿«ã€‚åœ¨åˆ†ç±»æ€§èƒ½æ–¹é¢ï¼ŒæŠ¥å‘Šäº†ä¸å…¶ä»–åº“ç›¸ä¼¼çš„å‡†ç¡®æ€§ã€‚},
  isbn = {978-3-319-99977-7 978-3-319-99978-4},
  langid = {english},
  keywords = {/unread},
  annotation = {ğŸ·ï¸ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/J5QGPIIG/Wicht et al. - 2018 - DLL A Fast Deep Neural Network Library.pdf}
}

@article{jorda_performance_2019,
  title = {Performance {{Evaluation}} of {{cuDNN Convolution Algorithms}} on {{NVIDIA Volta GPUs}}},
  shorttitle = {{{cuDNNå·ç§¯ç®—æ³•åœ¨NVIDIA Volta GPUä¸Šçš„æ€§èƒ½è¯„ä¼°}}},
  author = {Jorda, Marc and {Valero-Lara}, Pedro and Pena, Antonio J.},
  year = {2019},
  journal = {IEEE Access},
  volume = {7},
  pages = {70461--70473},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2019.2918851},
  urldate = {2024-04-29},
  abstract = {Convolutional neural networks (CNNs) have recently attracted considerable attention due to their outstanding accuracy in applications, such as image recognition and natural language processing. While one advantage of the CNNs over other types of neural networks is their reduced computational cost, faster execution is still desired for both training and inference. Since convolution operations pose most of the execution time, multiple algorithms were and are being developed with the aim of accelerating this type of operations. However, due to the wide range of convolution parameter configurations used in the CNNs and the possible data type representations, it is not straightforward to assess in advance which of the available algorithms will be the best performing in each particular case. In this paper, we present a performance evaluation of the convolution algorithms provided by the cuDNN, the library used by most deep learning frameworks for their GPU operations. In our analysis, we leverage the convolution parameter configurations from widely used the CNNs and discuss which algorithms are better suited depending on the convolution parameters for both 32 and 16-bit floating-point (FP) data representations. Our results show that the filter size and the number of inputs are the most significant parameters when selecting a GPU convolution algorithm for 32-bit FP data. For 16-bit FP, leveraging specialized arithmetic units (NVIDIA Tensor Cores) is key to obtain the best performance. ã€æ‘˜è¦ç¿»è¯‘ã€‘ç”±äºå·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰åœ¨å›¾åƒè¯†åˆ«å’Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰åº”ç”¨ä¸­çš„å‡ºè‰²å‡†ç¡®æ€§ï¼Œæœ€è¿‘å¼•èµ·äº†ç›¸å½“å¤§çš„å…³æ³¨ã€‚è™½ç„¶CNNç›¸å¯¹äºå…¶ä»–ç±»å‹ç¥ç»ç½‘ç»œçš„ä¸€ä¸ªä¼˜åŠ¿æ˜¯å®ƒä»¬é™ä½äº†è®¡ç®—æˆæœ¬ï¼Œä½†å¯¹äºè®­ç»ƒå’Œæ¨ç†æ¥è¯´ï¼Œä»ç„¶éœ€è¦æ›´å¿«çš„æ‰§è¡Œé€Ÿåº¦ã€‚ç”±äºå·ç§¯æ“ä½œå æ®äº†å¤§éƒ¨åˆ†æ‰§è¡Œæ—¶é—´ï¼Œå› æ­¤è¿‡å»å’Œç°åœ¨éƒ½åœ¨å¼€å‘å¤šç§ç®—æ³•ï¼Œç›®çš„æ˜¯åŠ é€Ÿè¿™ç§ç±»å‹çš„æ“ä½œã€‚ç„¶è€Œï¼Œç”±äºCNNä¸­ä½¿ç”¨çš„å·ç§¯å‚æ•°é…ç½®èŒƒå›´å¹¿æ³›ï¼Œä»¥åŠå¯èƒ½çš„æ•°æ®ç±»å‹è¡¨ç¤ºï¼Œæå‰è¯„ä¼°å“ªäº›å¯ç”¨ç®—æ³•åœ¨æ¯ä¸ªç‰¹å®šæƒ…å†µä¸‹æ€§èƒ½æœ€ä½³å¹¶ä¸ç®€å•ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¯¹cuDNNæä¾›çš„å·ç§¯ç®—æ³•è¿›è¡Œäº†æ€§èƒ½è¯„ä¼°ï¼ŒcuDNNæ˜¯å¤§å¤šæ•°æ·±åº¦å­¦ä¹ æ¡†æ¶ç”¨äºGPUæ“ä½œçš„åº“ã€‚åœ¨æˆ‘ä»¬çš„åˆ†æä¸­ï¼Œæˆ‘ä»¬åˆ©ç”¨äº†å¹¿æ³›ä½¿ç”¨çš„CNNçš„å·ç§¯å‚æ•°é…ç½®ï¼Œå¹¶æ ¹æ®32ä½å’Œ16ä½æµ®ç‚¹ï¼ˆFPï¼‰æ•°æ®è¡¨ç¤ºçš„å·ç§¯å‚æ•°è®¨è®ºäº†å“ªäº›ç®—æ³•æ›´é€‚åˆã€‚æˆ‘ä»¬çš„ç»“æœè¡¨æ˜ï¼Œåœ¨ä¸º32ä½FPæ•°æ®é€‰æ‹©GPUå·ç§¯ç®—æ³•æ—¶ï¼Œè¿‡æ»¤å™¨å¤§å°å’Œè¾“å…¥æ•°é‡æ˜¯æœ€é‡è¦çš„å‚æ•°ã€‚å¯¹äº16ä½FPï¼Œåˆ©ç”¨ä¸“ç”¨ç®—æœ¯å•å…ƒï¼ˆNVIDIAå¼ é‡æ ¸å¿ƒï¼‰æ˜¯è·å¾—æœ€ä½³æ€§èƒ½çš„å…³é”®ã€‚},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  lccn = {3},
  keywords = {/unread},
  annotation = {ğŸ·ï¸ /unread},
  file = {/Users/precious/Library/CloudStorage/OneDrive-Personal/zotero/storage/C6DU8DQG/Jorda et al. - 2019 - Performance Evaluation of cuDNN Convolution Algorithms on NVIDIA Volta GPUs.pdf}
}

@inproceedings{nazir_interpretable_2023,
  author = {Nazir, Zhumakhan and Yarovenko, Vladislav and Park, Jurn-Gyu},
  title = {Interpretable ML enhanced CNN Performance Analysis of cuBLAS, cuDNN and TensorRT},
  year = {2023},
  isbn = {9781450395175},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3555776.3578729},
  doi = {10.1145/3555776.3578729},
  abstract = {Deep learning models such as convolutional neural networks (CNNs) have a wide range of perception applications in image classification and object detection. However, despite the same CNN architectures, inference performance is different from implementations of specific libraries such as cuBLAS, cuDNN, and TensorRT. To investigate the performance effects of the state-of-the-art GPU libraries, this paper performs a case study of comparison and performance analysis of cuBLAS, cuDNN, and TensorRT implementations/libraries on YOLOv4-tiny, introducing crucial nvprof metrics for fair comparison and rationales of different performance and proposing interpretable machine learning (ML) model-based analysis. The results of our interpretable ML models show 100\% accuracy in the classification and 0.0094 MAPE in the regression tasks respectively.},
  booktitle = {Proceedings of the 38th ACM/SIGAPP Symposium on Applied Computing},
  pages = {1260â€“1265},
  numpages = {6},
  keywords = {interpretable ML, TensorRT, cuDNN, cuBLAS, GPU libraries, CNNs, deep learning},
  location = {Tallinn, Estonia},
  series = {SAC '23}
}

@inproceedings{papineni-etal-2002-bleu,
    title = "{B}leu: a Method for Automatic Evaluation of Machine Translation",
    author = "Papineni, Kishore  and
      Roukos, Salim  and
      Ward, Todd  and
      Zhu, Wei-Jing",
    editor = "Isabelle, Pierre  and
      Charniak, Eugene  and
      Lin, Dekang",
    booktitle = "Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2002",
    address = "Philadelphia, Pennsylvania, USA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/P02-1040",
    doi = "10.3115/1073083.1073135",
    pages = "311--318",
}

@inproceedings{banerjee-lavie-2005-meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    editor = "Goldstein, Jade  and
      Lavie, Alon  and
      Lin, Chin-Yew  and
      Voss, Clare",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W05-0909",
    pages = "65--72",
}

@inproceedings{lin-2004-rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/W04-1013",
    pages = "74--81",
}
